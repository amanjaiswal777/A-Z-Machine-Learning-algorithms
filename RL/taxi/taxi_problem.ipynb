{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "taxi_problem.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg8sKnLzY3cK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc-XsfJtZAYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "484455cd-53a3-49b4-9eda-40331a0d1878"
      },
      "source": [
        "env = gym.make(\"Taxi-v2\") # Create environment\n",
        "env.render() # Show it"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6RQcbVDZEVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f9a84f6b-a35e-450f-8c9d-98c2b2b8b4c5"
      },
      "source": [
        "# Number of possible actions\n",
        "action_size = env.action_space.n \n",
        "print(\"Action size \", action_size) \n",
        "\n",
        "# Number of possible states\n",
        "state_size = env.observation_space.n \n",
        "print(\"State size \", state_size)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action size  6\n",
            "State size  500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04aHVSb8ZPd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7832873a-6f9d-40ea-9735-65e099db0c41"
      },
      "source": [
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF52blWYZVvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "episodes = 30000            # Total episodes\n",
        "max_steps = 1000            # Max steps per episode\n",
        "lr = 0.3                    # Learning rate\n",
        "decay_fac = 0.00001         # Decay learning rate each iteration\n",
        "gamma = 0.90                # Discounting rate - later rewards impact less"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvVP254HZb80",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "81e2046e-8c6e-471e-a707-a472c8cb7fb2"
      },
      "source": [
        "for episode in range(episodes):\n",
        "    \n",
        "    state = env.reset() # Reset the environment\n",
        "    done = False        # Are we done with the environment\n",
        "    lr -= decay_fac     # Decaying learning rate\n",
        "    step = 0\n",
        "    \n",
        "    if lr <= 0: # Nothing more to learn?\n",
        "        break\n",
        "        \n",
        "    for step in range(max_steps):\n",
        "        \n",
        "        # Randomly Choose an Action\n",
        "        action = env.action_space.sample()\n",
        "        \n",
        "        # Take the action -> observe new state and reward\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        # Update qtable values\n",
        "        if done == True: # If last, do not count future accumulated reward\n",
        "            if(step < 199 | step > 201):\n",
        "                qtable[state, action] = qtable[state, action]+lr*(reward+gamma*0-qtable[state,action])\n",
        "            break\n",
        "        else: # Consider accumulated reward of best decision stream\n",
        "            qtable[state, action] = qtable[state,action]+lr*(reward+gamma*np.max(qtable[new_state,:])-qtable[state,action])\n",
        "    \n",
        "        # if done.. jump to next episode\n",
        "        if done == True:\n",
        "            break\n",
        "        \n",
        "        # moving states\n",
        "        state = new_state\n",
        "        \n",
        "    episode += 1\n",
        "    \n",
        "    if (episode % 3000 == 0):\n",
        "        print('episode = ', episode)\n",
        "        print('learning rate = ', lr)\n",
        "        print('-----------')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode =  3000\n",
            "learning rate =  0.26999999999997\n",
            "-----------\n",
            "episode =  6000\n",
            "learning rate =  0.23999999999993998\n",
            "-----------\n",
            "episode =  9000\n",
            "learning rate =  0.20999999999990998\n",
            "-----------\n",
            "episode =  12000\n",
            "learning rate =  0.17999999999987998\n",
            "-----------\n",
            "episode =  15000\n",
            "learning rate =  0.14999999999984998\n",
            "-----------\n",
            "episode =  18000\n",
            "learning rate =  0.11999999999982693\n",
            "-----------\n",
            "episode =  21000\n",
            "learning rate =  0.08999999999983856\n",
            "-----------\n",
            "episode =  24000\n",
            "learning rate =  0.059999999999848445\n",
            "-----------\n",
            "episode =  27000\n",
            "learning rate =  0.029999999999839697\n",
            "-----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MaYAh0MZtez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ff69eb5-758b-44ab-d95d-ed4cb0fae75b"
      },
      "source": [
        "# New environment\n",
        "state = env.reset()\n",
        "env.render()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while(done == False):\n",
        "    \n",
        "    action = np.argmax(qtable[state,:]) # Choose best action (Q-table)\n",
        "    state, reward, done, info = env.step(action) # Take action\n",
        "    total_reward += reward  # Summing rewards\n",
        "    \n",
        "    # Display it\n",
        "    time.sleep(0.5)\n",
        "    #clear_output(wait=True)\n",
        "    env.render()\n",
        "    print('Episode Reward = ', total_reward)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "Episode Reward =  -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "Episode Reward =  -2\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "Episode Reward =  -3\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "Episode Reward =  -4\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[42mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "Episode Reward =  -5\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Episode Reward =  -6\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Episode Reward =  -7\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Episode Reward =  -8\n",
            "+---------+\n",
            "|R: | :\u001b[42m_\u001b[0m:\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Episode Reward =  -9\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "Episode Reward =  -10\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode Reward =  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2An1gl9aNNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}